{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_comment', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "reddit_Data = pd.read_csv('Reddit_Data.csv') \n",
    "\n",
    "# Print the column names\n",
    "print(reddit_Data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_dict(words):\n",
    "    sorted_words = sorted(set(words))  # Sort words and remove duplicates\n",
    "    word2Ind = {word: i for i, word in enumerate(sorted_words)}\n",
    "    Ind2word = {i: word for i, word in enumerate(sorted_words)}\n",
    "    return word2Ind, Ind2word  # Manually retyped to remove U+00A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:                                             clean_comment  category\n",
      "0       family mormon have never tried explain them t...         1\n",
      "1      buddhism has very much lot compatible with chr...         1\n",
      "2      seriously don say thing first all they won get...        -1\n",
      "3      what you have learned yours and only yours wha...         0\n",
      "4      for your own benefit you may want read living ...         1\n",
      "...                                                  ...       ...\n",
      "37244                                              jesus         0\n",
      "37245  kya bhai pure saal chutiya banaya modi aur jab...         1\n",
      "37246              downvote karna tha par upvote hogaya          0\n",
      "37247                                         haha nice          1\n",
      "37248             facebook itself now working bjp’ cell          0\n",
      "\n",
      "[37249 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Load stopwords\n",
    "\n",
    "def clean_comment(reddit_Data):\n",
    "    if not isinstance(reddit_Data, str):  # Check if data is not a string\n",
    "        reddit_Data = \"\"  # Replace NaN or non-string values with an empty string\n",
    "    reddit_Data = reddit_Data.lower()  # Convert to lowercase\n",
    "    reddit_Data = re.sub(r'http\\S+|www\\S+', '', reddit_Data)\n",
    "    reddit_Data = emoji.replace_emoji(reddit_Data, replace='')  # Remove emojis\n",
    "    reddit_Data = re.sub(r'[^a-zA-Z0-9\\s]', '', reddit_Data)  # Remove special characters (except spaces)\n",
    "    reddit_Data = re.sub(r'\\s+', ' ', reddit_Data).strip()  # Remove extra spaces\n",
    "    return reddit_Data\n",
    "\n",
    "\n",
    "print(f'After cleaning:  {reddit_Data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' family mormon have never tried explain them they still stare puzzled from time time like some kind strange creature nonetheless they have come admire for the patience calmness equanimity acceptance and compassion have developed all the things buddhism teaches ', 'buddhism has very much lot compatible with christianity especially considering that sin and suffering are almost the same thing suffering caused wanting things shouldn want going about getting things the wrong way christian this would mean wanting things that don coincide with god will and wanting things that coincide but without the aid jesus buddhism could also seen proof god all mighty will and omnipotence certainly christians are lucky have one such christ there side but what about everyone else well many christians believe god grace salvation and buddhism god way showing grace upon others would also help study the things jesus said and see how buddha has made similar claims such rich man getting into heaven joke basically advocating that should rid ourselves material possessions fact distinctly remembered jesus making someone cry because that someone asked what achieve salvation and jesus replied with live like buddhist very very roughly translated also point out that buddha rarely spoke anything about god theory personally because knew well enough leave that jesus and mohamed who came later just remember conflict difference opinion but education can fun involving and enlightening easier teach something than prove right like intelligent design ', 'seriously don say thing first all they won get its too complex explain normal people anyway and they are dogmatic then doesn matter what you say see mechante post and for any reason you decide later life move from buddhism and that doesn suit you identity though you still get keep all the wisdom then your family will treat you like you went through weird hippy phase for while there didncha and you never hear the end pro tip don put one these your wall jpg ', 'what you have learned yours and only yours what you want teach different focus the goal not the wrapping paper buddhism can passed others without word about the buddha ', 'for your own benefit you may want read living buddha living christ thich nhat hanh you might find any subsequent discussions with your loved ones easier you are able articulate some the parallels that exist between buddhism and christianity don surprised they react negatively for having lost you treat them with compassion and deserved understanding although they may indeed display signs being hurt your new path properly sharing with them way that may alleviate their fear something they may perceive wrong the very least alien their beliefs may help allowing them the long run accept although not necessarily agree with your decision regardless where they end you have make your own way ']\n",
      "[1, 1, -1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Extract text from the correct column\n",
    "train_texts = reddit_Data['clean_comment'].tolist()\n",
    "train_labels = reddit_Data['category'].tolist()  # Assuming 'category' is the sentiment label\n",
    "\n",
    "# Print first few samples to verify\n",
    "print(train_texts[:5])\n",
    "\n",
    "print(train_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_comment', 'category'], dtype='object')\n",
      "['family mormon have never tried explain them they still stare puzzled from time time like some kind strange creature nonetheless they have come admire for the patience calmness equanimity acceptance and compassion have developed all the things buddhism teaches', 'buddhism has very much lot compatible with christianity especially considering that sin and suffering are almost the same thing suffering caused wanting things shouldn want going about getting things the wrong way christian this would mean wanting things that don coincide with god will and wanting things that coincide but without the aid jesus buddhism could also seen proof god all mighty will and omnipotence certainly christians are lucky have one such christ there side but what about everyone else well many christians believe god grace salvation and buddhism god way showing grace upon others would also help study the things jesus said and see how buddha has made similar claims such rich man getting into heaven joke basically advocating that should rid ourselves material possessions fact distinctly remembered jesus making someone cry because that someone asked what achieve salvation and jesus replied with live like buddhist very very roughly translated also point out that buddha rarely spoke anything about god theory personally because knew well enough leave that jesus and mohamed who came later just remember conflict difference opinion but education can fun involving and enlightening easier teach something than prove right like intelligent design', 'seriously don say thing first all they won get its too complex explain normal people anyway and they are dogmatic then doesn matter what you say see mechante post and for any reason you decide later life move from buddhism and that doesn suit you identity though you still get keep all the wisdom then your family will treat you like you went through weird hippy phase for while there didncha and you never hear the end pro tip don put one these your wall jpg', 'what you have learned yours and only yours what you want teach different focus the goal not the wrapping paper buddhism can passed others without word about the buddha', 'for your own benefit you may want read living buddha living christ thich nhat hanh you might find any subsequent discussions with your loved ones easier you are able articulate some the parallels that exist between buddhism and christianity don surprised they react negatively for having lost you treat them with compassion and deserved understanding although they may indeed display signs being hurt your new path properly sharing with them way that may alleviate their fear something they may perceive wrong the very least alien their beliefs may help allowing them the long run accept although not necessarily agree with your decision regardless where they end you have make your own way']\n"
     ]
    }
   ],
   "source": [
    "# Print column names to check correct names\n",
    "print(reddit_Data.columns)\n",
    "\n",
    "# Ensure 'clean_comment' column exists and contains only strings\n",
    "reddit_Data['clean_comment'] = reddit_Data['clean_comment'].fillna(\"\").astype(str)\n",
    "\n",
    "# Split dataset into train and test sets (if not already split)\n",
    "train_texts = reddit_Data['clean_comment'].tolist()  # ✅ Train set\n",
    "test_texts = reddit_Data['clean_comment'].tolist()   # ✅ Test set (if not separately defined)\n",
    "reddit_texts = reddit_Data['clean_comment'].tolist()  # ✅ Twitter texts\n",
    "\n",
    "# Apply cleaning function\n",
    "train_texts = [clean_comment(text) for text in train_texts]\n",
    "test_texts = [clean_comment(text) for text in test_texts]\n",
    "reddit_texts = [clean_comment(text) for text in reddit_texts]\n",
    "\n",
    "# Print samples to check correctness\n",
    "print(train_texts[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenization & Padding Complete!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define vocabulary size and max sequence length\n",
    "VOCAB_SIZE = 20000  # Max number of words in vocabulary\n",
    "MAX_LENGTH = 200  # Max length of sequences (truncated or padded)\n",
    "\n",
    "# Ensure texts are loaded\n",
    "if 'train_texts' not in locals() or 'test_texts' not in locals() or 'reddit_texts' not in locals():\n",
    "    print(\"Error: One or more text datasets (train_texts, test_texts, reddit_texts) are not defined!\")\n",
    "else:\n",
    "    # Tokenizer: Fit on combined dataset\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(train_texts + reddit_texts)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "    reddit_sequences = tokenizer.texts_to_sequences(reddit_texts)\n",
    "\n",
    "    # Pad sequences to ensure uniform shape\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "    twitter_padded = pad_sequences(reddit_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "    print(\"✅ Tokenization & Padding Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python3.11\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RNN Model Created Successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Model parameters\n",
    "EMBEDDING_DIM = 128  # Size of word embeddings\n",
    "RNN_UNITS = 64  # Number of RNN units\n",
    "\n",
    "# Ensure training data exists\n",
    "if 'train_padded' not in locals() or 'train_labels' not in locals():\n",
    "    print(\"Error: Training data (train_padded) or labels (train_labels) are not defined!\")\n",
    "else:\n",
    "    # Define RNN model\n",
    "    rnn_model = Sequential([\n",
    "        Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH, trainable=True),\n",
    "        SimpleRNN(RNN_UNITS, activation='tanh', return_sequences=False),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Model summary\n",
    "    rnn_model.summary()\n",
    "\n",
    "    print(\"✅ RNN Model Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract text and labels\n",
    "reddit_texts = reddit_Data['clean_comment'].tolist()\n",
    "reddit_labels = reddit_Data['category'].values  # Adjust this column name if necessary\n",
    "\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    reddit_texts, reddit_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert labels to NumPy arrays for model compatibility\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_comment', 'category'], dtype='object')\n",
      "[ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "print(reddit_Data.columns)\n",
    "print(reddit_Data['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_padded shape: (37249, 200)\n",
      "train_labels shape: (29799,)\n",
      "test_padded shape: (37249, 200)\n",
      "test_labels shape: (7450,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_padded shape: {train_padded.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "print(f\"test_padded shape: {test_padded.shape}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 37249\n",
      "Unique labels: [ 1 -1  0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total dataset size: {len(reddit_Data)}\")\n",
    "print(f\"Unique labels: {reddit_Data['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 29799, Test size: 7450\n",
      "Train labels: 29799, Test labels: 7450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert text & labels to lists\n",
    "texts = reddit_Data['clean_comment'].tolist()\n",
    "labels = reddit_Data['category'].values  # Ensure labels match texts\n",
    "\n",
    "# Correct train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}, Test size: {len(test_texts)}\")\n",
    "print(f\"Train labels: {len(train_labels)}, Test labels: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_padded shape: (29799, 200), train_labels shape: 29799\n",
      "test_padded shape: (7450, 200), test_labels shape: 7450\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts + test_texts)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad sequences\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# Check new sizes\n",
    "print(f\"train_padded shape: {train_padded.shape}, train_labels shape: {len(train_labels)}\")\n",
    "print(f\"test_padded shape: {test_padded.shape}, test_labels shape: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in train_labels: [-1  0  1]\n",
      "Unique values in test_labels: [-1  0  1]\n",
      "NaN in train_labels: 0\n",
      "NaN in test_labels: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in train_labels:\", np.unique(train_labels))\n",
    "print(\"Unique values in test_labels:\", np.unique(test_labels))\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"NaN in train_labels:\", np.isnan(train_labels).sum())\n",
    "print(\"NaN in test_labels:\", np.isnan(test_labels).sum())\n",
    "\n",
    "\n",
    "\n",
    "# Remove NaN labels\n",
    "valid_indices = ~np.isnan(train_labels)\n",
    "train_labels = train_labels[valid_indices]\n",
    "train_padded = train_padded[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.where(train_labels == -1, 0, train_labels)\n",
    "test_labels = np.where(test_labels == -1, 0, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in train_labels after cleaning: [0 1]\n",
      "Unique values in test_labels after cleaning: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in train_labels after cleaning:\", np.unique(train_labels))\n",
    "print(\"Unique values in test_labels after cleaning:\", np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = np.array(train_padded, dtype=np.float32)\n",
    "test_padded = np.array(test_padded, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m932/932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 38ms/step - accuracy: 0.6059 - loss: 0.6548 - val_accuracy: 0.6372 - val_loss: 0.6282\n",
      "Epoch 2/5\n",
      "\u001b[1m932/932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.7709 - loss: 0.5005 - val_accuracy: 0.7101 - val_loss: 0.5592\n",
      "Epoch 3/5\n",
      "\u001b[1m932/932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.8474 - loss: 0.3824 - val_accuracy: 0.7538 - val_loss: 0.5248\n",
      "Epoch 4/5\n",
      "\u001b[1m932/932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.8878 - loss: 0.3036 - val_accuracy: 0.7667 - val_loss: 0.5422\n",
      "Epoch 5/5\n",
      "\u001b[1m932/932\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - accuracy: 0.9098 - loss: 0.2598 - val_accuracy: 0.7709 - val_loss: 0.5578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19e6b0530d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(train_padded, train_labels, epochs=5, batch_size=32, validation_data=(test_padded, test_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
